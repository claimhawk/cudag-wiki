# Visualizer Project Overview

## 1. Overview

The **Visualizer** project is an interactive web-based tool designed to visualize attention patterns generated by Qwen3-VL LoRA adapters. It enables users to inspect how different LoRA adapters influence the model’s attention mechanisms when processing visual inputs. This is particularly useful for researchers and developers fine-tuning vision-language models, as it provides insight into which regions of an image the model focuses on at different transformer layers and across different adapter configurations.

The core problem this tool solves is the lack of transparency in attention mechanisms for vision-language models with LoRA adapters. Without visualization, it’s difficult to understand how adapter modifications alter the model’s focus, making debugging, comparison, and model interpretation challenging.

## 2. Architecture

The project is organized into three distinct layers:

- **Frontend (Next.js)**: Handles user interaction, rendering, and UI components.
- **Backend (FastAPI)**: Manages API endpoints, orchestrates inference, and serves data to the frontend.
- **Inference Backend (Modal GPU)**: Runs heavy computational tasks (e.g., attention extraction) on GPU instances via Modal, a serverless compute platform.

The architecture follows a clean separation of concerns, with each layer communicating via HTTP APIs. The frontend communicates with the backend via REST endpoints, and the backend delegates inference tasks to the Modal GPU environment.

### Directory Structure

```
visualizer/
├── backend/                 # FastAPI backend
│   └── app.py               # Main FastAPI application entry point
├── modal/                   # Modal GPU inference environment
│   └── attention.py         # Attention extraction logic for Qwen3-VL
├── frontend/                # Next.js frontend
│   ├── next-env.d.ts        # TypeScript type definitions for Next.js
│   ├── next.config.ts       # Next.js configuration
│   ├── src/
│   │   ├── app/
│   │   │   ├── layout.tsx   # Root layout component
│   │   │   └── page.tsx     # Main page component
│   │   ├── components/
│   │   │   ├── ImageUpload.tsx     # Uploads image for analysis
│   │   │   ├── AttentionHeatmap.tsx # Renders attention heatmaps
│   │   │   ├── LayerSelector.tsx    # Selects transformer layers
│   │   │   └── AdapterSelector.tsx  # Selects LoRA adapters
│   ├── package.json         # Frontend dependencies
│   └── tsconfig.json        # TypeScript configuration
├── README.md                # Project-level documentation
├── CODE_QUALITY.md          # Code quality guidelines
├── CLAUDE.md                # Claude-specific documentation (if applicable)
└── frontend/README.md       # Frontend-specific documentation
```

## 3. Key Components

### Backend (`backend/app.py`)

This is the main FastAPI application that exposes the API endpoints. It handles:

- Receiving image and adapter parameters from the frontend.
- Orchestrating inference via Modal (via `modal/attention.py`).
- Returning attention maps and metadata to the frontend.

Key endpoints:
- `POST /api/analyze`: Initiates inference and returns attention maps.
- `GET /api/adapters`: Returns a list of available LoRA adapters.
- `GET /api/layers`: Returns metadata about transformer layers and heads.

### Inference Engine (`modal/attention.py`)

This module runs on Modal’s GPU environment and is responsible for:

- Loading the base Qwen3-VL model and LoRA adapters.
- Extracting attention weights during inference.
- Upsampling attention maps to match the original image resolution.
- Returning structured attention data (layer, head, heatmap) to the backend.

It uses `attn_implementation="eager"` to ensure attention outputs are available for extraction — this is incompatible with `flash_attention_2`.

### Frontend Components

#### `frontend/src/components/ImageUpload.tsx`

Handles image file upload via a drag-and-drop interface or file input. Validates image format and size, then sends the image to the backend for analysis.

#### `frontend/src/components/AttentionHeatmap.tsx`

Renders the attention heatmap overlaid on the original image. Accepts attention data from the backend and uses React’s `canvas` or `img` elements to visualize heatmaps with color gradients.

#### `frontend/src/components/LayerSelector.tsx`

A dropdown or slider component that allows users to select which transformer layer to visualize. Communicates with the backend to fetch layer metadata via `/api/layers`.

#### `frontend/src/components/AdapterSelector.tsx`

A dropdown component that lets users select between different LoRA adapters. Fetches adapter list from `/api/adapters` and triggers re-analysis when a new adapter is selected.

#### `frontend/src/app/page.tsx`

The main page component that orchestrates the UI. It renders the upload component, selector components, and heatmap. It manages state (e.g., selected adapter, layer, image) and coordinates API calls.

## 4. Data Flow

1. **User Uploads Image** → `ImageUpload.tsx` captures the image file.
2. **User Selects Adapter & Layer** → `AdapterSelector.tsx` and `LayerSelector.tsx` send selections to the frontend state.
3. **Frontend Triggers Analysis** → `page.tsx` sends a POST request to `/api/analyze` with image and adapter parameters.
4. **Backend Orchestrates Inference** → `app.py` calls `modal/attention.py` via Modal to run inference.
5. **Attention Data Returned** → Backend returns structured attention data (layer, head, heatmap) to frontend.
6. **Frontend Renders Heatmap** → `AttentionHeatmap.tsx` renders the heatmap overlaid on the image.
7. **Optional Comparison** → If comparing base model vs. adapter, backend may return two sets of attention maps for side-by-side visualization.

## 5. Getting Started

### Prerequisites

- Python 3.9+
- Node.js 18+
- `uv` (for dependency management)
- `modal` CLI (for GPU inference)
- GPU-enabled environment (for Modal)

### Setup

#### 1. Clone the Repository

```bash
git clone https://github.com/your-org/visualizer.git
cd visualizer
```

#### 2. Set Up Backend (FastAPI)

```bash
cd backend
uv venv && source .venv/bin/activate
uv pip install -r requirements.txt
uvicorn app:app --reload
```

> Note: `requirements.txt` should include `fastapi`, `uvicorn`, `modal`, `transformers`, `torch`, `pillow`, and other dependencies.

#### 3. Set Up Frontend (Next.js)

```bash
cd frontend
npm install
npm run dev
```

> Ensure `next.config.ts` is configured for TypeScript and any custom middleware or API routes.

#### 4. Set Up Modal Inference Environment

Ensure you have the Modal CLI installed:

```bash
pip install modal
```

The `modal/attention.py` file defines the Modal function that runs the inference. You can test it locally or deploy it via:

```bash
modal deploy modal/attention.py
```

> The backend will automatically use this deployed function via the Modal API.

#### 5. Test the Application

- Open `http://localhost:3000` in your browser.
- Upload an image.
- Select an adapter and layer.
- View the attention heatmap.

#### 6. API Testing (Optional)

Use `curl` or Postman to test endpoints:

```bash
# List adapters
curl http://localhost:8000/api/adapters

# Get layer metadata
curl http://localhost:8000/api/layers

# Run inference
curl -X POST http://localhost:8000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"image_path": "/path/to/image.jpg", "adapter_name": "adapter1", "layer": 12}'
```

## Notes

- The system is designed to be scalable: Modal handles GPU-heavy tasks, while the frontend remains lightweight.
- Attention maps are upsampled to match the original image resolution for accurate visualization.
- The `attn_implementation="eager"` setting is required to enable attention output — this may impact performance but ensures compatibility with LoRA adapter analysis.
- All components are modular and can be extended or replaced independently.

This project provides a powerful, interactive tool for understanding how LoRA adapters influence attention in vision-language models — ideal for research, debugging, and model interpretation.